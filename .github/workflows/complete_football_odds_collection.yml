# .github/workflows/complete_football_odds_collection.yml
name: Complete Football Odds Collection (TOUS les marchés et bookmakers)

# Déclenchement manuel uniquement (suppression du schedule)
on:
  workflow_dispatch:  # Permet de lancer manuellement le workflow
    inputs:
      league_filter:
        description: 'Filtrer par ligue (optionnel, ex: ENG1,FRA1) - laissez vide pour toutes'
        required: false
        default: ''
      batch_size:
        description: 'Taille des batches (défaut: 25)'
        required: false
        default: '25'

jobs:
  collect-complete-odds:
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 heures max pour la collecte complète
    
    steps:
    # Étape 1: Checkout du code
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0  # Récupère tout l'historique

    # Étape 2: Configuration de Python
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Étape 3: Installation des dépendances
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests pandas numpy
        pip install --upgrade urllib3 requests

    # Étape 4: Création des dossiers nécessaires dans data/
    - name: Create directories
      run: |
        mkdir -p data
        mkdir -p data/odds
        mkdir -p data/odds/raw_data
        mkdir -p data/odds/summaries

    # Étape 5: Vérification de l'espace disque et des fichiers existants
    - name: Check system resources and existing files
      run: |
        echo "=== VÉRIFICATION DES RESSOURCES SYSTÈME ==="
        df -h
        echo ""
        echo "=== MÉMOIRE DISPONIBLE ==="
        free -h
        echo ""
        echo "=== FICHIERS EXISTANTS (MATCHS) ==="
        if [ -d "data" ]; then
          find data/ -maxdepth 1 -name "*.csv" -type f | head -10 || echo "Aucun fichier CSV match dans data/"
          echo "Nombre de fichiers CSV matchs: $(find data/ -maxdepth 1 -name '*.csv' -type f | wc -l)"
        fi
        echo ""
        echo "=== FICHIERS EXISTANTS (ODDS) ==="
        if [ -d "data/odds/raw_data" ]; then
          ls -la data/odds/raw_data/ || echo "Dossier odds vide"
          echo "Nombre de fichiers CSV odds: $(ls data/odds/raw_data/*.csv 2>/dev/null | wc -l)"
        fi

    # Étape 6: Test de connectivité API
    - name: Test API connectivity
      env:
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      run: |
        echo "🔌 Test de connectivité API..."
        python -c "
import requests
import os
headers = {
    'x-rapidapi-host': 'api-football-v1.p.rapidapi.com',
    'x-rapidapi-key': os.environ['RAPIDAPI_KEY']
}
try:
    response = requests.get('https://api-football-v1.p.rapidapi.com/v3/status', headers=headers, timeout=10)
    if response.status_code == 200:
        data = response.json()
        print('✅ API accessible')
        print(f'Account: {data.get(\"account\", {})}')
        print(f'Subscription: {data.get(\"subscription\", {})}')
    else:
        print(f'⚠️ Status code: {response.status_code}')
except Exception as e:
    print(f'❌ Erreur API: {e}')
"

    # Étape 7: Configuration des paramètres
    - name: Configure collection parameters
      run: |
        echo "LEAGUE_FILTER=${{ github.event.inputs.league_filter || '' }}" >> $GITHUB_ENV
        echo "BATCH_SIZE=${{ github.event.inputs.batch_size || '25' }}" >> $GITHUB_ENV
        echo "🎯 Paramètres de collecte:"
        echo "  - Filtrage ligues: '${{ github.event.inputs.league_filter || 'TOUTES' }}'"
        echo "  - Taille batch: ${{ github.event.inputs.batch_size || '25' }}"

    # Étape 8: Modification dynamique du script si filtrage
    - name: Prepare collection script
      run: |
        if [ ! -z "$LEAGUE_FILTER" ]; then
          echo "🔧 Configuration du filtrage par ligue: $LEAGUE_FILTER"
          python -c "
import os
# Lire le contenu du script
with open('complete_football_odds_collector.py', 'r') as f:
    content = f.read()

# Modifier la configuration des ligues si filtrage demandé
league_filter = os.environ.get('LEAGUE_FILTER', '')
if league_filter:
    leagues = [l.strip() for l in league_filter.split(',') if l.strip()]
    print(f'Filtrage activé pour: {leagues}')
    
    # Injecter le filtrage dans le script
    filter_code = f'''
        # Filtrage dynamique des ligues (depuis GitHub Actions)
        if True:  # Filtrage activé
            filtered_leagues = {leagues}
            self.all_leagues = {{k: v for k, v in self.all_leagues.items() if k in filtered_leagues}}
            logger.info(f\"🎯 Filtrage activé: {{len(self.all_leagues)}} ligues sélectionnées: {{list(self.all_leagues.keys())}}\")
        '''
    
    # Injecter après l'initialisation des ligues
    content = content.replace(
        \"# Saisons à analyser\",
        filter_code + \"\\n        # Saisons à analyser\"
    )
    
    # Sauvegarder le script modifié
    with open('complete_football_odds_collector_filtered.py', 'w') as f:
        f.write(content)
    
    print('✅ Script filtré créé: complete_football_odds_collector_filtered.py')
else:
    print('ℹ️ Aucun filtrage - utilisation du script original')
"
        else
          echo "ℹ️ Pas de filtrage - copie du script original"
          cp complete_football_odds_collector.py complete_football_odds_collector_filtered.py
        fi

    # Étape 9: Exécution de la collecte complète des odds
    - name: Run complete football odds collection
      env:
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      run: |
        echo "🚀 DÉBUT DE LA COLLECTE COMPLÈTE DES ODDS"
        echo "📅 Date de début: $(date)"
        echo "🎯 Objectif: TOUS les bookmakers, TOUS les marchés"
        echo "📁 Destination: data/odds/"
        echo ""
        
        # Lancement avec gestion d'erreurs
        python complete_football_odds_collector_filtered.py 2>&1 | tee collection_output.log
        
        echo ""
        echo "📅 Date de fin: $(date)"

    # Étape 10: Vérification des données collectées
    - name: Verify and analyze collected odds data
      run: |
        echo "=== VÉRIFICATION DES DONNÉES COLLECTÉES ==="
        echo "📅 Vérification effectuée le: $(date)"
        echo ""
        
        # Vérifier le dossier principal
        if [ -d "data/odds" ]; then
          echo "✅ Dossier principal odds créé dans data/"
          
          # Analyser les fichiers de données brutes
          if [ -d "data/odds/raw_data" ]; then
            echo ""
            echo "📊 FICHIERS DE DONNÉES BRUTES:"
            raw_files=(data/odds/raw_data/*.csv)
            if [ -e "${raw_files[0]}" ]; then
              total_files=0
              total_size=0
              total_lines=0
              
              for file in data/odds/raw_data/*.csv; do
                if [ -f "$file" ]; then
                  filename=$(basename "$file")
                  lines=$(($(wc -l < "$file") - 1))  # Exclure l'en-tête
                  size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "0")
                  size_mb=$(echo "scale=2; $size / 1024 / 1024" | bc -l 2>/dev/null || echo "0")
                  
                  echo "  📄 $filename:"
                  echo "      - Lignes: $lines cotes"
                  echo "      - Taille: ${size_mb}MB"
                  
                  total_files=$((total_files + 1))
                  total_lines=$((total_lines + lines))
                  total_size=$((total_size + size))
                fi
              done
              
              total_size_mb=$(echo "scale=2; $total_size / 1024 / 1024" | bc -l 2>/dev/null || echo "0")
              echo ""
              echo "📊 RÉSUMÉ TOTAL:"
              echo "  - Fichiers générés: $total_files"
              echo "  - Total cotes collectées: $total_lines"
              echo "  - Taille totale: ${total_size_mb}MB"
            else
              echo "⚠️ Aucun fichier CSV dans raw_data"
            fi
          fi
          
          # Analyser les résumés
          if [ -d "data/odds/summaries" ]; then
            echo ""
            echo "📈 FICHIERS DE RÉSUMÉS:"
            summary_files=(data/odds/summaries/*.csv)
            if [ -e "${summary_files[0]}" ]; then
              ls -la data/odds/summaries/
            else
              echo "⚠️ Aucun fichier de résumé généré"
            fi
          fi
          
          # Vérifier les métadonnées
          if [ -f "data/odds/collection_metadata.json" ]; then
            echo ""
            echo "📄 MÉTADONNÉES DE COLLECTE:"
            echo "$(head -20 data/odds/collection_metadata.json)"
          fi
          
        else
          echo "❌ Dossier data/odds non créé"
        fi
        
        # Vérifier les logs
        echo ""
        echo "📝 LOGS DE COLLECTE (dernières lignes):"
        if [ -f "football_odds.log" ]; then
          tail -20 football_odds.log
        else
          echo "Aucun fichier de log trouvé"
        fi

    # Étape 11: Gestion intelligente des commits (éviter les fichiers trop gros)
    - name: Intelligent commit and push strategy
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        
        echo "🔍 Analyse des fichiers pour commit intelligent..."
        
        # Ajouter les fichiers de résumés (toujours petits)
        git add data/odds/summaries/*.csv 2>/dev/null || echo "Aucun fichier de résumé"
        git add data/odds/collection_metadata.json 2>/dev/null || echo "Aucun fichier de métadonnées"
        git add football_odds.log 2>/dev/null || echo "Aucun fichier de log"
        
        # Gestion intelligente des gros fichiers de données
        large_files_count=0
        small_files_count=0
        
        if [ -d "data/odds/raw_data" ]; then
          for file in data/odds/raw_data/*.csv; do
            if [ -f "$file" ]; then
              size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "0")
              size_mb=$(echo "$size / 1024 / 1024" | bc -l 2>/dev/null || echo "0")
              size_mb_int=$(echo "$size_mb" | cut -d. -f1)
              
              if [ "$size_mb_int" -gt 50 ]; then
                # Fichier > 50MB : créer un résumé au lieu de commit complet
                echo "📊 Fichier volumineux détecté (${size_mb}MB): $(basename "$file")"
                large_files_count=$((large_files_count + 1))
                
                # Créer un fichier résumé pour les gros fichiers
                echo "league_file,total_odds,file_size_mb,created_at" > "data/odds/large_file_summary.csv"
                echo "$(basename "$file"),$(($(wc -l < "$file") - 1)),${size_mb},$(date -Iseconds)" >> "data/odds/large_file_summary.csv"
                
              else
                # Fichier < 50MB : commit normal
                echo "✅ Ajout fichier normal (${size_mb}MB): $(basename "$file")"
                git add "$file"
                small_files_count=$((small_files_count + 1))
              fi
            fi
          done
        fi
        
        # Ajouter le résumé des gros fichiers s'il existe
        git add data/odds/large_file_summary.csv 2>/dev/null || true
        
        # Vérifier s'il y a des changements à committer
        if git diff --staged --quiet; then
          echo "✅ Aucun changement détecté - données déjà à jour"
        else
          echo "🎲 Nouvelles données odds détectées - commit en cours..."
          
          # Compter les fichiers modifiés
          modified_files=$(git diff --staged --name-only | wc -l)
          
          # Message de commit détaillé
          commit_message="🎲 Collecte complète odds historiques - $(date '+%Y-%m-%d %H:%M')

📊 Résumé de la collecte:
- Fichiers modifiés: $modified_files
- Petits fichiers ajoutés: $small_files_count
- Gros fichiers résumés: $large_files_count
- Période: 365 derniers jours
- Objectif: TOUS les bookmakers et marchés
- Destination: data/odds/

🤖 Collecte manuelle via GitHub Actions"
          
          git commit -m "$commit_message"
          git push
          
          echo "✅ Données odds mises à jour et poussées vers le repository"
        fi

    # Étape 12: Upload des logs et gros fichiers comme artifacts
    - name: Upload large files and logs as artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: complete-odds-collection-${{ github.run_number }}
        path: |
          data/odds/raw_data/*.csv
          football_odds.log
          collection_output.log
          data/odds/collection_metadata.json
        retention-days: 30

    # Étape 13: Résumé final et notifications
    - name: Final summary and notification
      if: always()
      run: |
        echo ""
        echo "🎉 === RÉSUMÉ FINAL DE LA COLLECTE COMPLÈTE ==="
        echo "📅 Exécution terminée le: $(date)"
        echo ""
        
        # Statut de la collecte
        if [ ${{ job.status }} = "success" ]; then
          echo "✅ STATUT: SUCCÈS"
        else
          echo "❌ STATUT: ÉCHEC"
        fi
        
        # Résumé des fichiers créés
        if [ -d "data/odds/raw_data" ]; then
          file_count=$(ls data/odds/raw_data/*.csv 2>/dev/null | wc -l)
          echo "📊 Fichiers de données créés: $file_count"
        fi
        
        if [ -d "data/odds/summaries" ]; then
          summary_count=$(ls data/odds/summaries/*.csv 2>/dev/null | wc -l)
          echo "📈 Fichiers de résumés créés: $summary_count"
        fi
        
        # Espace disque utilisé
        if [ -d "data/odds" ]; then
          total_size=$(du -sh data/odds 2>/dev/null | cut -f1 || echo "0")
          echo "💾 Espace total utilisé: $total_size"
        fi
        
        # Informations sur les artifacts
        echo ""
        echo "📦 ARTIFACTS DISPONIBLES:"
        echo "  - Nom: complete-odds-collection-${{ github.run_number }}"
        echo "  - Contenu: Tous les fichiers de données + logs"
        echo "  - Rétention: 30 jours"
        echo "  - Accès: Actions > Artifacts de cette exécution"
        
        echo ""
        echo "📁 STRUCTURE FINALE DES DONNÉES:"
        echo "data/"
        echo "├── [fichiers de matchs existants].csv"
        echo "└── odds/"
        echo "    ├── raw_data/"
        echo "    │   └── [ligue]_complete_odds.csv"
        echo "    ├── summaries/"
        echo "    │   ├── [ligue]_bookmaker_summary.csv"
        echo "    │   └── [ligue]_bet_types_summary.csv"
        echo "    └── collection_metadata.json"
        
        echo ""
        echo "🎯 COLLECTE COMPLÈTE TERMINÉE!"
        echo "   Toutes les cotes historiques disponibles ont été récupérées"
        echo "   pour la période de 365 derniers jours sur TOUS les marchés"
        echo "   et TOUS les bookmakers disponibles via API-Football."
        echo "   📁 Données organisées dans: data/odds/" 
